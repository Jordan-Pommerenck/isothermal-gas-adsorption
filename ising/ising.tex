\documentclass[letterpaper,twocolumn,amsmath,amssymb,pre,aps,10pt]{revtex4-1}
\usepackage{graphicx} % Include figure files
\usepackage{color}
\usepackage{nicefrac} % Include for inline fractions

\usepackage{xargs}                      % Use more than one optional parameter in a new commands
\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
\usepackage[colorinlistoftodos,prependcaption,textsize=normalsize]{todonotes}
\usepackage{mdframed}
\usepackage{braket}

% define colors for comments
\definecolor{dark-gray}{gray}{0.10}
\definecolor{light-gray}{gray}{0.70}

\newcommand{\red}[1]{{\bf \color{red} #1}}
\newcommand{\green}[1]{{\bf \color{green} #1}}
\newcommand{\blue}[1]{{\bf \color{blue} #1}}
\newcommand{\cyan}[1]{{\bf \color{cyan} #1}}

\newcommand{\davidsays}[1]{{\color{red} [\green{David:} \emph{#1}]}}
\newcommand{\jpsays}[1]{{\color{red} [\blue{Jordan:} \emph{#1}]}}
\newcommandx{\jpcom}[2][1=inline]{\todo[linecolor=gray,backgroundcolor=light-gray,bordercolor=dark-gray,#1]{\textbf{Jordan says:} #2} }
\begin{document}

\title{Flat-histogram method comparison on 2D Ising Model
}

\author{Jordan K. Pommerenck} \author{David Roundy}
\affiliation{Department of Physics, Oregon State University,
  Corvallis, OR 97331}

\begin{abstract}
  We examine the convergence of the Stochastic Approximation with a Dynamic
  update factor (SAD) algorithm applied to the 2D Ising model. Comparison with
  SAMC and WL methods show that SAD performs robustly and without user input
  knowledge of an energy range. We also compare SAD with a final production run WL simulation with fixed weights. While SAD is more powerful in the
  common case in which the range of energies is not known in advance, the 2D Ising model energy range is readily known and easily discovered by WL methods.
\end{abstract}

\maketitle

\section{Introduction}
Flat-histogram Monte-Carlo simulation algorithms calculate the thermodynamic
properties of various systems over a range of temperatures.  The first histogram
method used a single canonical Monte Carlo simulation to predict properties for
nearby temperatures~\cite{ferrenberg1988new}. While the method effectively
samples a narrow energy range given limited by a narrow temperature range, it
proves computationally inefficient at sampling large energy ranges.

Multicanonical methods, introduced by Berg and Neuhaus, enabled flat-histogram
sampling which improved the exploration of configuration space and allowed the
system to overcome free-energy barriers~\cite{berg1991multicanonical, berg1992multicanonical}.
These works led to increase in the development of a variety of ``flat'' (or ``broad'') histogram
methods~\cite{penna1996broad, penna1998broad, swendsen1999transition,
wang2001determining, wang2001efficient} which could explore a wider range
of energies.  In addition to obtaining thermodynamic information for the entire
energy range for a single simulation, these approaches cannot be easily trapped
in a local energy minimum like a canonical simulation.


Wang and Landau introduced one of the most widely used flat-histogram
Monte-Carlo algorithms that accurately determined the density of states (DOS) for
a statistical system~\cite{wang2001determining,wang2001efficient}. For all of
its power, the method unfortunately requires a priori knowledge of several
user-defined parameters. Thus, for any given system under study, the user needs
to determine the ideal parameters in order to apply the method. The Wang-Landau
algorithm is also known to violate detailed balance (although only for brief
time intervals)~\cite{yan2003fast, shell2002generalization}. With the violation
of detailed balance, convergence of the algorithm is not guaranteed.

Because of the uncertainty of convergence for WL, many studies were undertaken
to understand how the modification (or update) factor $\gamma$ impacts the
convergence~\cite{zhou2005understanding,lee2006convergence,
belardinelli2007wang}. The ideal choice of decreasing the modification factor
helps to prevent CPU time from being wasted by continuing to perform
calculations after the error in the density of states is
saturated~\cite{belardinelli2008analysis}. Modification factors that decrease
faster than $1/t$ were found to lead to
non-convergence~\cite{belardinelli2007fast}.  These findings led to the
formation of the $1/t$-WL algorithm and also sparked some debate as to whether
the simulation should be split into multiple parts, called ``production run" WL, in order to preserve ergodicity and
detailed balance~\cite{jayasri2005wang, mukhopadhyay2008monte}.

Liang independently considered whether WL could be treated as a special case of
Stochastic Approximation whose convergence could be mathematically
proven~\cite{liang2006theory, liang2007stochastic}. In 2007, Liang et
al.~\cite{liang2007stochastic} argued that WL can be considered a form of
Stochastic Approximation Monte Carlo (SAMC). Unlike WL, SAMC can guarantee
convergence (if certain conditions are met). Despite the added benefit of
guaranteed convergence, the method still has a system specific user-defined
variable. Such variables often create difficulty when applying Monte-Carlo
methods across arbitrary systems.

Kim \emph{et al.} introduced Statistical Temperature Monte Carlo (STMC) and the
related Statistical Temperature Molecular Dynamics (STMD), an adaption
of the WL method that approximates the entropy as a piecewise linear function,
which improves convergence for systems with a continuously varying
energy~\cite{kim2006statistical, kim2007statistical}. STMC applied to WL
requires a temperature range be specified rather than an energy range.  Kim
\emph{et al.} extended this work as Replica Exchange Statistical Temperature
Monte Carlo (RESTMC), which uses replica exchange of multiple overlapping STMC
simulations to improve convergence~\cite{kim2009replica}. Recently,
Junghans \emph{et al.} demonstrated a close connection between
metadynamics, which was introduced by Laio and
Parinello~\cite{laio2002escaping}, and WL-based Monte Carlo methods, with STMD
forging the connection~\cite{junghans2014molecular}.

The SAD (Stochastic Approximation with
Dynamic $\gamma$) method outlined by Pommerenck
et.al~\cite{pommerenck2020stochastic} is a special version of the SAMC algorithm
that dynamically chooses the modification factor rather than relying on system
dependent parameters. SAD shares the same convergence properties with SAMC while
replacing un-physical user-defined parameters with the algorithms dynamic
choice.

In this work, we apply the family of weight-based flat-histogram Monte
Carlo methods (WL, 1/t-WL, SAMC, SAD) to the 2D Ising model.

\section{Ising Model}
The 2D Ising spin-lattice system is widely used as a testbed when
benchmarking or comparing Monte-Carlo
methods~\cite{ferdinand1969bounded, wang1999transition, trebst2004optimizing, barash2019estimating}. The 2nd order
phase transition behavior and the ability to directly calculate the
exact solution for finite lattices~\cite{beale1996exact, haggkvist2004computation} make the
system sufficiently interesting for such theoretical comparisons. It is
also important to note that direct comparison of the other methods can
be made with WL as its original implementation was done on this
system~\cite{wang2001determining,wang2001efficient}.
We test the convergence of several flat-histogram methods
on the periodic 2D square lattice ferromagnetic Ising model with nearest
neighbor interactions~\cite{landau2004new}.
\begin{align}
\mathcal{H} = -\sum_{\braket{i,j}} \sigma_i \sigma_j - h \sum_i s_i
\end{align}
The $N\times N$ spin system can take on values of $\sigma_i = \pm 1$
for up or down spins respectively. In the absence of a magnetic field ($h =
0$), We can write the Hamiltonian as follows~\cite{onsager1944crystal,
kaufman1949crystal}:
\begin{align}
\mathcal{H} = -\sum_{\braket{i,j}} \sigma_i \sigma_j
\end{align}
where the sum is over nearest neighbor spin sites. Beale showed that for finite
lattices the Density of States could directly be calculated from the partition
function~\cite{beale1996exact}
\begin{align}
Z = \sum_E g(E) e^{-{\beta E}}
\end{align}
% where $g(E)$ is the multiplicity of the system which is proportional to
% $D(E)$. We can estimate the average deviation from the exact solution
% using the relation~\cite{schneider2017convergence, shakirov2018convergence,
% barash2017control}:
% \begin{align}
% \braket{\epsilon(t)}_E = \frac{1}{N_E - 1} \sum_E \left( \frac{S(E,t) - S_{\text{Beale}(E)}}{S_{\text{Beale}(E)}} \right)
% \end{align}
where $g(E)$ is the multiplicity of the system which is proportional to
$D(E)$. We can estimate the maximum deviation in the canonical specific heat capacity $c_V$ from the exact solution~\cite{schneider2017convergence, shakirov2018convergence,
barash2017control,barash2017gpu}:
\begin{align}
\braket{\epsilon(t)}_E = \max_E \left( c_V(E,S,T,t) - c_V(E,S,T,t)_{\text{Beale}}\right)
\end{align}
% As per the implementation by Wang and
% Landau~\cite{wang2001determining}, the total number of available energy
% states are $N-1$.
Computing the specific heat capacity $c_V$ presents a difficult challenge for
any Monte-Carlo method due to fluctuations in the derivative of the internal energy.  Methods that accurately compute $c_V$ also by extension accurately compute the internal energy.

\section{Flat-histogram methods}\label{sec:histogram}
Flat-histogram methods determine the density of states $D(E)$ over a broad range
of energies by simulating each energy with equivalent accuracy. Flat-histogram
Monte Carlo methods propose randomly chosen ``moves'' which change the state of
the system and must satisfy detailed balance.  Each algorithm differs in how it
determines the probability of accepting a move and in what additional statistics
must be collected in order decide on that probability.

We describe several closely related flat-histogram methods which each rely on a
weight function $w(E)$ to determine $D(E)$.  For these algorithms, the
probability of accepting a move is given by
\begin{equation}
	\mathcal{P}(E_\text{old} \rightarrow E_\text{new})
	= \min\left[1,\frac{w(E_\text{old})}{w(E_\text{new})}\right]
\end{equation}
which biases the simulation in favor of energies with low weights. The result of
weights $w(E)$ that are proportional to $D(E)$ is an entirely flat-histogram. We
can relate the entropy to the weights in the microcanonical ensemble, since the
entropy is defined as $S(E) \equiv k_B\ln(D(E)) \approx \ln w(E)$.

Flat-histogram methods employ a random walk in energy space to estimate $D(E)$.  Each method operates by continuously updating the weights at each
step of the simulation
\begin{equation}
	\ln{w_{t+1}(E)}=\ln{w_{t}(E)}
	+\gamma_t
\end{equation}
where $t$ is number of the current move, $\gamma(t)$ is a move-dependent update
factor, and $E$ is the current energy.  This update causes the random walk to
avoid frequently sampling the same energies, leading to a rapid exploration
of energy space. Flat-histogram methods differ primarily in how they schedule
the decrease of $\gamma_t$.

The Wang-Landau algorithm~\cite{wang2001efficient,wang2001determining,
landau2014guide} explores energy space by setting $\gamma_{t=0}^{\text{WL}}=1$,and then decreases $\gamma^{\text{WL}}$ in prescribed stages. An energy range of
interest must be specified~\cite{wang2001efficient, schulz2003avoiding,yan2003fast}, which often requires multiple simulations if unknown.

The number (``counts'') of moves ending at each energy are stored in a
histogram.  For a sufficiently flat energy histogram (typically user-specified to be 0.8), $\gamma^{\text{WL}}$ is decreased by a specified factor of $\frac12$ and the histogram is reset to zero. The entire process is repeateduntil $\gamma^{\text{WL}}$ reaches a desired cutoff.

Zhou et. al also showed that the WL algorithm never converges
exponentially~\cite{zhou2008optimal} and succesfully bounded the statistical
error between $t^{-\frac12}$ and $1/t$~\cite{zhou2008optimal}. He also suggests
that a modification factor of $1/t$ is optimal for WL.

The $1/t$-WL method is a refinement of the original implementation that
addresses the error saturation of WL. Belardinelli and Pereyra
implemented a schedule~\cite{belardinelli2007fast} that enforced that
if $\gamma < \nicefrac{1}{t}$ then the update factor is set to
$\nicefrac{1}{t}$ and the histogram is no longer tracked. Using this
metric, the error saturation is avoided since the correct density of
states is approached asymptotically as
$t^{-\frac12}$~\cite{belardinelli2008analysis}.
Schneider $\text{et. al}$ outline minor refinements to the $1/t$-WL algorithm
including $\nicefrac{N_S}{t}$ scaling and switching from standard WL to
$1/t$-WL when the update factor $\gamma <
\nicefrac{N_S}{t}$~\cite{schneider2017convergence}. As per the original
$1/t$ implementation~\cite{belardinelli2007fast}, the update factor is
decreased once all $N_S$ energy states are visited at least once (i.e.
$H(E) \neq 0$) effectively avoiding the concept of `flatness'.

The WL method can also be implemented as part of a production run which aids in
equilibration~\cite{gross2018massively}. WL is used to generate the weights
resulting in a flat-histogram~\cite{janke2017generalized}. Once a user-defined
minimum $\gamma$ is reached, the simulation switches to a final production run
with fixed weights. WL implemented in this way can ensure ergodicity and
detailed-balance; however, the convergence of simulation is still impacted by
the choice of the minimum $\gamma$.

Another weight-based flat-histogram method is
the Stochastic Approximation Monte Carlo (SAMC) algorithm. SAMC has a simple
schedule by which the update factor $\gamma$ is continuously
decreased~\cite{liang2007stochastic, werlich2015stochastic,
schneider2017convergence}.  The update factor is defined in the
original implementation~\cite{liang2007stochastic} in terms of an
arbitrary tunable parameter $t_0$,
\begin{align}
\gamma_{t}^{\text{SA}} =\frac{t_0}{\max(t_0,t)}
\end{align}
where as above $t$ is the number of moves that have been attempted.

SAMC offers extreme simplicity and proven convergence.
Provided the update factor satisfies
\begin{align}
\sum_{t=1}^\infty \gamma_{t} = \infty \quad\textrm{and}\quad
\sum_{t=1}^\infty \gamma_{t}^\zeta < \infty
\end{align}
where $\zeta \in \{1,2\}$, Liang has shown that the weights are proven
to converge to the true density of states~\cite{liang2006theory,
liang2007stochastic}.  Unlike WL methods, the energy range need not be
known \emph{a priori.}  The convergenc time depends only on the choice of
parameter $t_0$.
The parameter $t_0$ can unfortunately be difficult to chose in advance
for arbitrary systems.
Liang \emph{et al.} give a rule of thumb in
which $t_0$ is chosen in the range from $2N_S$ to $100N_S$ where $N_S$
is the number of energy bins~\cite{liang2007stochastic}.  Schneider
\emph{et al.} found and we confirm that for the Ising model this heuristic is helpful for small spin systems, but that larger systems require an even higher
$t_0$ value~\cite{schneider2017convergence}.

Pommerenck et al. propose a refinement~\cite{pommerenck2020stochastic} to SAMC
where the update factor is determined dynamically rather than by the user.
Stochastic Approximation with a dynamic $\gamma$ (SAD) requires the user to
provide the lowest temperature of interest $T_{\min}$. This is analogous to WL
requiring a priori an energy range of interest; however, this may easier to
identify and is more physical than the SAMC parameter $t_0$. The update factor
can be written in terms of the current estimates for the highest $E_H$ and
lowest $E_L$ energies of interest and the last time that an energy in the range
of interest is encountered $t_L$.
\begin{align}
  \gamma_{t}^{\text{SAD}} =
     \frac{
       \frac{E_{H}-E_{L}}{T_{\text{min}}} + \frac{t}{t_L}
     }{
       \frac{E_{H}-E_{L}}{T_{\text{min}}} + \frac{t}{N_S}\frac{t}{t_L}
     }
\end{align}
SAD only explores the energy range of interest as specified by the minimum
temperature of interest $T_{\min} < T < \infty$. During the simulation the two
energies $E_H$ and $E_L$, are refined such that the range of energies are conservatively
estimated. The weights are calculated for each energy region according to the original
prescription.
\begin{enumerate}
\item {$E < E_L$:} $w(E>E_H) = w(E_H)$
\item {$E_L < E < E_H$:} moves are handled the same as other weight-based
methods that are mentioned
\item {$E > E_H$:} $w(E<E_L) = w(E_L)e^{-\frac{E_L-E}{T_{\min}}}$
\end{enumerate}
Each time the simulation changes the value of $E_H$ or $E_L$, the weights
within the new portion of the interesting energy range are updated.

\section{Results}

\jpsays{this all needs rewritten in terms of heat capacity and WL min gamma.}
We tested the algorithms on two 2D Ising model systems.  The first is a smaller
simulation with a lattic size of 32 and the second has a lattice size of 128.
For each system we use a reasonable root-mean-square displacement distance
$\delta_0 = 0.05\sigma$ for proposed moves. The simulations explore the energy
space of the systems with minimum reduced temperatures of $T_{\text{min}} = 1$
for simulations All simulations lead to the minimum important energy $E_{\min}$
and maximum entropy energy $E_{\max}$ being calculated (with the exception of
the WL methods where both of these parameters are needed a priori).

\begin{figure}
\includegraphics[width=\columnwidth]{gamma-n32.pdf}
  \caption{
  The update factor $\gamma_t$ versus the iteration number for the $N=32 \times 32$
  system.}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{N32-Cv-error-default.pdf}
  \caption{The maximum error in the heat capacity for each method for the $N=32 \times 32$ and $T_{\min} = 1$ as a function of number of iterations run.  The maximum error is averaged over 8 independent simulations, and the best and worst simulations for each method are shown as a semi-transparent shaded area.}
  \label{fig:N32-cv-error}
\end{figure}

% \begin{figure}
% \includegraphics[width=\columnwidth]{N32-entropy-error-default.pdf}
%   \caption{
%   The average entropy error for each MC method for $N=32 \times 32$,
%                $\delta_0 = 0.05\sigma$, and $T_{\min} = 1$
%                as a function of number of iterations run.  The error is
%                averaged over 8 independent simulations, and the best
%                and worst simulations for each method are shown as a
%                semi-transparent shaded area.}\label{fig:n32}
% \end{figure}

\subsection{The 32 $\times$ 32 Ising model}
For this simulation, we chose a minimum reduced temperature of $1$, which
corresponds to an interesting energy range from $-2048$ to $48$.  The number of
important energy states for this system is $N_S = 529$.

Fig.~\ref{fig:n32} shows the average error in the entropy as a function of time for
this system with the displacement distance of $\delta_0 = 0.05$. The solid/dashed lines
represent the average of the absolute value of the error in the entropy averaged
over eight simulations using different random number seeds. The range of average
errors for each simulation is shown as a shaded region around its mean error. By
the time $10^8$ moves have been made all but the WL simulation have begun to
converge as $1/\sqrt{t}$. We then see the WL error saturate around $10^9$ moves.

\begin{figure}
\includegraphics[width=\columnwidth]{gamma-n128.pdf}
  \caption{
  The update factor $\gamma_t$ versus the iteration number for the $N=128 \times 128$
  system.}
\end{figure}

\begin{figure}
  \includegraphics[width=\columnwidth]{N128-Cv-error-default.pdf}
  \caption{The maximum error in the heat capacity for each method for the $N=128 \times 128$ and $T_{\min} = 1$ as a function of number of iterations run.  The maximum error is averaged over 8 independent simulations, and the best and worst simulations for each method are shown as a semi-transparent shaded area.}
  \label{fig:N128-cv-error}
\end{figure}

% \begin{figure}
% \includegraphics[width=\columnwidth]{N128-entropy-error-default.pdf}
%   \caption{
%   The average entropy error for each MC method for $N=128 \times 128$,
%                $\delta_0 = 0.05\sigma$, and $T_{\min} = 1$
%                as a function of number of iterations run.  The error is
%                averaged over 8 independent simulations, and the best
%                and worst simulations for each method are shown as a
%                semi-transparent shaded ar.}\label{fig:n128}
% \end{figure}

\subsection{128 $\times$ 128 Ising system}
For this simulation, we chose a minimum reduced temperature of $1$, which
corresponds to an interesting energy range from $-32768$ to $100$.  The number of
important energy states for this system is $N_S = 8214$~
\jpsays{This is wrong! This is simulation range. This section needs rewritten.  Why is displacement distance even mentioned?}.
Fig.~\ref{fig:n128} shows the average error in the entropy as a function of time
for this system with the displacement distance of $\delta_0 = 0.05$. The solid/dashed
lines represent the average of the absolute value of the error in the entropy
averaged over eight simulations using different random number seeds. The range
of average errors for each simulation is shown as a shaded region around its
mean error. By the time $10^{12}$ moves have been made all but the WL and SAD
simulations have begun to converge as $1/\sqrt{t}$. We then see the WL error
saturate around $10^{12}$ moves. The error in the SAD method will continue to
converge faster than $1/\sqrt{t}$ until the update factor approaches that of
1/t-WL at which point both methods will converge equivalently.

\subsection{Separate Heat Capacity Section?}
Why does the heat capacity error saturate (i.e. the error is around 1 out 
but the heat capacity value is much as 1600!)
The smaller system SAD does great and minGamma does great but minGamma needs
the user to pick the right minGamma for the production run otherwise you get
the 128 by 128 results.

We can either break this up or mix in?

Why are we looking at Cv?

(1) the errors in the derivative will be more pronounced. (numerical differentiation of noisy data)

(2) interested in computing thermodynamic quantities

(3) compare with published literature?


\section{Conclusion}
\jpsays{add something here about production run WL-minGamma
The method performs as well as 1/t-WL and SAD.  Unfortunately, the addition of another tunable parameter Gamma min makes the method somewhat more difficult to implement in practice.}
We have examined the convergence of SAD versus other widely used weight-based
histogram methods.  We have shown that Sad samples the energy space
corresponding to a desired range of temperatures for a variety of system sizes.
We find that both SAD and 1/t-WL demonstrate excellent and robust convergence.
They both converge more rapidly than SAMC, and unlike WL do not suffer from
error saturation. We find that for larger systems SAD reduces the updated factor
more slowly (and conservatively) than 1/t-WL. This means that SAD will take
proportionately more moves to converge to the same value as 1/t-WL as system
size is increased. SAD requires the user to specify a temperature range of
interest rather than an energy range of interest as 1/t-WL does. For use cases
in which a range of desired temperatures is known, this will make the SAD method
considerably more convenient.

\section{Acknowledgments}

We thank Klas Markstr\"{o}m for discussions regarding the exact computation of the partition function for the 2D Ising model. We also with to thank Johannes Zierenberg for helpful discussions regarding WL production run simulations and
Martin Weigel for discussions regarding entropic population annealing.

\bibliography{ising} % Produces the bibliography via BibTeX.

\end{document}
